# ğŸ“Œ Sentiment Analysis using Deep Learning (LSTM / GRU)

A complete **Natural Language Processing (NLP)** project that performs **sentiment analysis on IMDb movie reviews** using **Deep Learning (LSTM & GRU)** and compares results with a **traditional Machine Learning baseline (Logistic Regression)**.

This project is designed to demonstrate **endâ€‘toâ€‘end NLP workflow**, clean project structuring, and strong theoretical understandingâ€”making it suitable for **GitHub portfolios, CVs, and Masterâ€™s SOPs**.

---

## ğŸš€ Project Overview

**Goal:**
Classify IMDb movie reviews as **Positive** or **Negative** using deep learning models.

**Key Highlights:**

* Text preprocessing & tokenization
* Word embeddings
* Sequential deep learning models (LSTM & GRU)
* Comparison with Logistic Regression (ML baseline)
* Clear explanation of *why deep learning outperforms traditional ML for NLP*

---

## ğŸ§  Learning Outcomes

Through this project, I learned:

* How to preprocess raw text data for NLP tasks
* Tokenization, padding, and vocabulary management
* Understanding word embeddings
* Training LSTM and GRU models for sequence learning
* Handling vanishing gradient problems
* Comparing deep learning with traditional ML approaches

---

## ğŸ“‚ Dataset

* **IMDb Movie Reviews Dataset**
* Binary sentiment classification:

  * `positive` â†’ 1
  * `negative` â†’ 0

Dataset structure:

```
data/raw/imdb_reviews.csv
```

Columns:

* `review` â€“ movie review text
* `sentiment` â€“ sentiment label

---

## ğŸ›  Tech Stack

* **Programming Language:** Python
* **Deep Learning:** TensorFlow, Keras
* **NLP Tools:** NLTK
* **Machine Learning:** Scikitâ€‘learn
* **Visualization:** Matplotlib, Seaborn

---

## ğŸ“ Project Structure

```
text-classification-nlp/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â”‚   â””â”€â”€ imdb_reviews.csv
â”‚
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ exploratory.ipynb
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ preprocess.py
â”‚   â”œâ”€â”€ models.py
â”‚   â”œâ”€â”€ train.py
â”‚   â”œâ”€â”€ evaluate.py
â”‚   â””â”€â”€ baseline_ml.py
â”‚
â”œâ”€â”€ results/
â”‚   â””â”€â”€ models/
â”‚       â”œâ”€â”€ lstm_model.h5
â”‚       â””â”€â”€ gru_model.h5
â”‚
â”œâ”€â”€ main.py
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## âš™ï¸ How to Run the Project

### 1ï¸âƒ£ Clone the Repository

```bash
git clone https://github.com/yourusername/text-classification-nlp.git
cd text-classification-nlp
```

### 2ï¸âƒ£ Create Virtual Environment

```bash
python -m venv .venv
source .venv/bin/activate   # Linux / macOS
.venv\Scripts\activate      # Windows
```

### 3ï¸âƒ£ Install Dependencies

```bash
pip install -r requirements.txt
```

### 4ï¸âƒ£ Run the Project

```bash
python main.py
```

Models will be saved in:

```
results/models/
```

---

## ğŸ“Š Model Comparison

| Model               | Description                    | Strength                               |
| ------------------- | ------------------------------ | -------------------------------------- |
| Logistic Regression | TFâ€‘IDF based ML baseline       | Fast, interpretable                    |
| LSTM                | Long Shortâ€‘Term Memory network | Handles longâ€‘term dependencies         |
| GRU                 | Gated Recurrent Unit           | Faster & efficient alternative to LSTM |

---

## ğŸ§  Why LSTM / GRU over Traditional ML?

Traditional ML models (e.g., Logistic Regression):

* Treat text as independent features
* Lose word order and context

LSTM / GRU models:

* Preserve **sequential information**
* Learn **longâ€‘term dependencies** in text
* Capture sentiment patterns spread across sentences

---

## âš ï¸ Vanishing Gradient Problem (Explained)

* In standard RNNs, gradients shrink during backpropagation
* This prevents learning longâ€‘range dependencies

**LSTM & GRU solve this using gates:**

* Control what information to remember or forget
* Enable stable gradient flow over long sequences

---

## ğŸ““ Exploratory Notebook

The `exploratory.ipynb` notebook includes:

* Class distribution analysis
* Word clouds for positive & negative reviews
* Sequence length visualization
* Quick LSTM training
* Accuracy & loss plots

---

## ğŸ Conclusion

This project demonstrates:

* Strong understanding of NLP fundamentals
* Practical deep learning skills
* Clean software engineering practices
* Readiness for advanced studies or applied research in AI

---

â­ This project is intended for academic demonstration and postgraduate applications.

